<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
  background-color: white !important;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
  background-color: white !important;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Apache Spark","children":[{"content":"Core Components","children":[{"content":"<strong>Spark Core</strong>","children":[{"content":"Grundlegende Architektur von Spark","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"Ressourcenverwaltung und Scheduling","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"Fehlerbehandlung und Wiederholbarkeit von Jobs","children":[],"payload":{"tag":"li","lines":"14,15"}}],"payload":{"tag":"li","lines":"11,15"}},{"content":"<strong>RDD (Resilient Distributed Dataset)</strong>","children":[{"content":"Unterschiede zu DataFrames und Datasets","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"Transformationen und Aktionen","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Vorteile der Fehlertoleranz und Parallelit&#xe4;t","children":[],"payload":{"tag":"li","lines":"18,19"}}],"payload":{"tag":"li","lines":"15,19"}},{"content":"<strong>DAG (Directed Acyclic Graph)</strong>","children":[{"content":"Steuerung der Transformationen und deren Reihenfolge","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"Fehlerbehandlung und Neuberechnung von fehlgeschlagenen Tasks","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"Einfluss auf die Performance und Optimierung der Berechnungen","children":[],"payload":{"tag":"li","lines":"22,23"}}],"payload":{"tag":"li","lines":"19,23"}},{"content":"<strong>Shared Variables</strong>","children":[{"content":"Einsatz von Broadcast Variables zur Minimierung von Netzwerkverkehr","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"Einsatz von Accumulators f&#xfc;r Z&#xe4;hleroperationen in verteilten Berechnungen","children":[],"payload":{"tag":"li","lines":"25,26"}}],"payload":{"tag":"li","lines":"23,26"}},{"content":"<strong>Spark SQL</strong>","children":[{"content":"DataFrames","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Datasets","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"<strong>Catalyst Optimizer</strong>","children":[{"content":"Logical Plan Optimization","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"Predicate Pushdown","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Join Reordering","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Cost-Based Optimization (CBO)","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Code Generation (Tungsten Engine)","children":[],"payload":{"tag":"li","lines":"34,35"}}],"payload":{"tag":"li","lines":"29,35"}}],"payload":{"tag":"li","lines":"26,35"}},{"content":"<strong>Spark Streaming</strong>","children":[{"content":"DStream (Discretized Stream)","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"Structured Streaming","children":[],"payload":{"tag":"li","lines":"37,38"}}],"payload":{"tag":"li","lines":"35,38"}},{"content":"<strong>MLlib (Machine Learning Library)</strong>","children":[{"content":"Algorithms","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"Pipelines","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Feature Transformation","children":[],"payload":{"tag":"li","lines":"41,42"}}],"payload":{"tag":"li","lines":"38,42"}},{"content":"<strong>GraphX</strong>","children":[{"content":"Graph Processing","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"Pregel API","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"li","lines":"42,46"}}],"payload":{"tag":"h2","lines":"10,11"}},{"content":"Cluster Architecture","children":[{"content":"\n<p data-lines=\"47,48\"><strong>Master</strong></p>","children":[{"content":"Verantwortlich f&#xfc;r die Verwaltung von Ressourcen und die Verteilung von Tasks","children":[],"payload":{"tag":"li","lines":"48,49"}},{"content":"Entscheidet, auf welchem Worker-Node Tasks ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Verbindet und f&#xfc;gt neue Workers hinzu.","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"52,55\"><code data-lines=\"52,55\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-master.sh</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"52,55"}}],"payload":{"tag":"li","lines":"51,55"}}],"payload":{"tag":"li","lines":"47,55"}},{"content":"\n<p data-lines=\"55,56\"><strong>Worker</strong></p>","children":[{"content":"F&#xfc;hren die tats&#xe4;chliche Verarbeitung der Daten aus.","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Halten den Executor, der die Tasks ausf&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Werden auf Cluster-Nodes ausgef&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"60,63\"><code data-lines=\"60,63\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-slave.sh spark://&lt;master-spark-URL&gt;:7077</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"60,63"}}],"payload":{"tag":"li","lines":"59,63"}}],"payload":{"tag":"li","lines":"55,63"}},{"content":"\n<p data-lines=\"63,64\"><strong>Driver</strong></p>","children":[{"content":"Verwaltet den Spark-Job, koordiniert die Arbeit der Cluster-Ressourcen","children":[],"payload":{"tag":"li","lines":"64,65"}},{"content":"Stellt eine Verbindung zum Master her und verteilt Tasks","children":[],"payload":{"tag":"li","lines":"65,66"}}],"payload":{"tag":"li","lines":"63,66"}},{"content":"\n<p data-lines=\"66,67\"><strong>Executor</strong></p>","children":[{"content":"F&#xfc;hren Tasks auf Worker-Nodes aus und speichern Ergebnisse","children":[],"payload":{"tag":"li","lines":"67,68"}},{"content":"Jeder Executor f&#xfc;hrt Aufgaben f&#xfc;r den zugewiesenen Task aus und speichert Daten lokal","children":[],"payload":{"tag":"li","lines":"68,69"}}],"payload":{"tag":"li","lines":"66,69"}},{"content":"\n<p data-lines=\"69,70\"><strong>Cluster Manager</strong></p>","children":[{"content":"Verwalten des Ressourcenmanagements im Cluster","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"Der Cluster Manager &#xfc;bernimmt das Starten von Executorn und das Zuweisen von Ressourcen","children":[{"content":"\n<p data-lines=\"72,73\">Standalone</p>","children":[{"content":"Es sid keine zus&#xe4;tzlichen Abh&#xe4;ngigkeiten notwendig f&#xfc;r Deployment.","children":[],"payload":{"tag":"li","lines":"73,74"}},{"content":"Der schnellste Weg einen Cluster auszusetzen und laufen zu lassen.","children":[],"payload":{"tag":"li","lines":"74,75"}}],"payload":{"tag":"li","lines":"72,75"}},{"content":"\n<p data-lines=\"75,76\">Hadoop YARN</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"YARN-Cluster haben ihre eigenen Abh&#xe4;ngigkeiten.","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"Komplexer f&#xfc;r Deployments.","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"Unterst&#xfc;tzt weitere zus&#xe4;tzliche Frameworks:","children":[{"content":"Apache Spark","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"Apache MapReduce","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"Apache Tez","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Apache Flink","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"Apache HBase","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"Apache Storm","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Apache Drill","children":[],"payload":{"tag":"li","lines":"86,87"}}],"payload":{"tag":"li","lines":"79,87"}}],"payload":{"tag":"li","lines":"75,87"}},{"content":"\n<p data-lines=\"87,88\">Apache Mesos</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"88,89"}},{"content":"M&#xf6;glichkeit f&#xfc;r dynamische Partitionierungen zw. Spark und anderen Big-Data-Frameworks.","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"Skalierbar zwischen mehreren Spark-Instanzen.","children":[],"payload":{"tag":"li","lines":"90,91"}}],"payload":{"tag":"li","lines":"87,91"}},{"content":"\n<p data-lines=\"91,92\">Kubernetes</p>","children":[{"content":"Automatisierte Deployments.","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"Portierbare Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"Skaliert des Cluster.","children":[],"payload":{"tag":"li","lines":"94,95"}},{"content":"Vereinfachtes Abh&#xe4;ngigkeits-Management.","children":[],"payload":{"tag":"li","lines":"95,96"}}],"payload":{"tag":"li","lines":"91,96"}},{"content":"\n<p data-lines=\"96,97\"><strong>Spark-Shell</strong></p>","children":[{"content":"Interaktive Shell f&#xfc;r das schnelle Testen von Spark-Funktionen","children":[{"content":"Wird oft genutzt, um Spark-APIs zu lernen und schnelle Experimente durchzuf&#xfc;hren","children":[],"payload":{"tag":"li","lines":"98,99"}}],"payload":{"tag":"li","lines":"97,99"}},{"content":"Verf&#xfc;gbar f&#xfc;r Python und Scala.","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"Sobald Spark-Shell gestartet wurde, kann direkt mit Daten gearbeitet werden.","children":[],"payload":{"tag":"li","lines":"100,101"}},{"content":"Starte:<pre data-lines=\"102,105\"><code data-lines=\"102,105\">./bin/pyspark\n</code></pre>","children":[],"payload":{"tag":"li","lines":"101,106"}}],"payload":{"tag":"li","lines":"96,106"}},{"content":"\n<p data-lines=\"106,107\">Local Mode.</p>","children":[{"content":"L&#xe4;uft ohne Cluster-Manager","children":[],"payload":{"tag":"li","lines":"107,108"}},{"content":"Nutzt nur die lokale CPU und den Arbeitsspeicher der Maschine","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"Kann in verschiedenen Konfigurationen betrieben werden","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"Eignet sich gut f&#xfc;r Entwicklung, Debugging und kleine Datenmengen.","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Starte und konfiguriere:","children":[{"content":"\n<pre data-lines=\"112,117\"><code data-lines=\"112,117\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./bin/spark-submit \\\n    --master <span class=\"hljs-built_in\">local</span>[#] \\\n    &lt;additional configuration&gt;</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"112,118"}},{"content":"\n<p data-lines=\"118,119\"><strong><code>spark-submit</code></strong></p>","children":[{"content":"\n<p data-lines=\"119,120\">Kommandozeilen-Tool, mit dem Apache Spark-Anwendungen gestartet werden. Es erm&#xf6;glicht die Ausf&#xfc;hrung von Spark-Programmen in verschiedenen Modi (lokal oder auf einem Cluster) und mit verschiedenen Ressourcen-Konfigurationen.</p>","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"\n<p data-lines=\"120,121\">Das spark-submit-Skript liest zus&#xe4;tzliche spezifische Konfigurationen in <code>/conf/spark-defaults.conf</code></p>","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"\n<p data-lines=\"121,122\">Stellt ggf. eine Verbindung zum zum Cluster Manager her.</p>","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"\n<p data-lines=\"122,124\">Alle Anwendungsdateien (einschlie&#xdf;lich JARs oder Python-Dateien) m&#xfc;ssen angegeben werden, damit sie<br>\ndas Treiberprogramm starten und Dateien verteilen k&#xf6;nnen, die im Cluster ausgef&#xfc;hrt werden sollen.</p>","children":[],"payload":{"tag":"li","lines":"122,124"}},{"content":"\n<pre data-lines=\"124,133\"><code data-lines=\"124,133\"> spark-submit \\\n <span class=\"hljs-attr\">--master</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--deploy-mode</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--num-executors</span> &lt;anzahl&gt; \\\n <span class=\"hljs-attr\">--executor-memory</span> &lt;speicher&gt; \\\n <span class=\"hljs-attr\">--executor-cores</span> &lt;kerne&gt; \\\n mein_spark_programm<span class=\"hljs-selector-class\">.py</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"124,134"}},{"content":"\n<p data-lines=\"134,135\"><strong>Wichtige Parameter von `spark-submit</strong>:</p>","children":[{"content":"<strong><code>--master</code></strong>: Definiert, wo die Spark-App l&#xe4;uft (<code>local</code>, <code>yarn</code>, <code>kubernetes</code>, <code>mesos</code>)","children":[{"content":"Beispiel: <code>--master yarn</code>","children":[],"payload":{"tag":"li","lines":"136,137"}}],"payload":{"tag":"li","lines":"135,137"}},{"content":"<strong><code>--deploy-mode</code></strong>: <code>client</code> (Driver l&#xe4;uft lokal) oder <code>cluster</code> (Driver l&#xe4;uft im Cluster)","children":[{"content":"Beispiel: <code>--deploy-mode cluster</code>","children":[],"payload":{"tag":"li","lines":"138,139"}}],"payload":{"tag":"li","lines":"137,139"}},{"content":"<strong><code>--num-executors</code></strong>: Anzahl der Executors im Cluster","children":[{"content":"Beispiel: <code>--num-executors 5</code>","children":[],"payload":{"tag":"li","lines":"140,141"}}],"payload":{"tag":"li","lines":"139,141"}},{"content":"<strong><code>--executor-memory</code></strong>: Speicher pro Executor","children":[{"content":"Beispiel: <code>--executor-memory 4G</code>","children":[],"payload":{"tag":"li","lines":"142,143"}}],"payload":{"tag":"li","lines":"141,143"}},{"content":"<strong><code>--executor-cores</code></strong>: CPU-Kerne pro Executor","children":[{"content":"Beispiel: <code>--executor-cores 2</code>","children":[],"payload":{"tag":"li","lines":"144,145"}}],"payload":{"tag":"li","lines":"143,145"}},{"content":"<strong><code>--driver-memory</code></strong>: Speicher f&#xfc;r den Driver","children":[{"content":"Beispiel: <code>--driver-memory 2G</code>","children":[],"payload":{"tag":"li","lines":"146,147"}}],"payload":{"tag":"li","lines":"145,147"}},{"content":"<strong><code>--conf</code></strong>: Zus&#xe4;tzliche Konfigurationen setzen","children":[{"content":"Beispiel: <code>--conf spark.sql.shuffle.partitions=50</code>","children":[],"payload":{"tag":"li","lines":"148,149"}}],"payload":{"tag":"li","lines":"147,149"}},{"content":"Wildcard (*) nutzen f&#xfc;r ALLE CPU-Kerne. Ansonsten Anzahl (#) angeben.","children":[],"payload":{"tag":"li","lines":"149,152"}}],"payload":{"tag":"li","lines":"134,152"}}],"payload":{"tag":"li","lines":"118,152"}}],"payload":{"tag":"li","lines":"111,152"}}],"payload":{"tag":"li","lines":"106,152"}}],"payload":{"tag":"li","lines":"71,152"}}],"payload":{"tag":"li","lines":"69,152"}},{"content":"\n<p data-lines=\"152,153\"><strong>Task</strong></p>","children":[{"content":"Kleinste Ausf&#xfc;hrungseinheit, die von einem Executor verarbeitet wird","children":[],"payload":{"tag":"li","lines":"153,154"}},{"content":"Besteht aus einer Transformation und/oder einer Aktion","children":[],"payload":{"tag":"li","lines":"154,155"}}],"payload":{"tag":"li","lines":"152,155"}},{"content":"\n<p data-lines=\"155,156\"><strong>Job</strong></p>","children":[{"content":"Besteht aus mehreren Tasks, die von einem Driver koordiniert werden","children":[],"payload":{"tag":"li","lines":"156,157"}},{"content":"Jeder Job besteht aus einem DAG von Tasks, die auf verschiedenen Nodes ausgef&#xfc;hrt werden","children":[],"payload":{"tag":"li","lines":"157,159"}}],"payload":{"tag":"li","lines":"155,159"}}],"payload":{"tag":"h2","lines":"46,47"}},{"content":"Spark Konfigurationen","children":[{"content":"Statische vs. Dynamische Konfiguration","children":[{"content":"<strong>Statische Konfiguration</strong>","children":[{"content":"Wird <strong>vor dem Start</strong> von Spark gesetzt","children":[],"payload":{"tag":"li","lines":"164,165"}},{"content":"&#xc4;nderungen erfordern einen <strong>Neustart</strong>","children":[],"payload":{"tag":"li","lines":"165,166"}},{"content":"Typische Orte:","children":[{"content":"<code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"<code>spark-env.sh</code>","children":[],"payload":{"tag":"li","lines":"168,169"}},{"content":"<code>log4j.properties</code>","children":[],"payload":{"tag":"li","lines":"169,170"}}],"payload":{"tag":"li","lines":"166,170"}},{"content":"Beispiel (<code>spark-defaults.conf</code>):<pre data-lines=\"171,175\"><code data-lines=\"171,175\">spark.executor.memory 4g\nspark.driver.memory 2g\n</code></pre>","children":[],"payload":{"tag":"li","lines":"170,176"}}],"payload":{"tag":"h4","lines":"163,164"}},{"content":"<strong>Dynamische Konfiguration</strong>","children":[{"content":"Kann <strong>zur Laufzeit</strong> ge&#xe4;ndert werden","children":[],"payload":{"tag":"li","lines":"177,178"}},{"content":"Keine Neustarts erforderlich","children":[],"payload":{"tag":"li","lines":"178,179"}},{"content":"Typische Methoden:","children":[{"content":"<code>spark-submit --conf</code>","children":[],"payload":{"tag":"li","lines":"180,181"}},{"content":"<code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"Dynamische Ressourcenallokation (<code>spark.dynamicAllocation.enabled</code>)","children":[],"payload":{"tag":"li","lines":"182,183"}}],"payload":{"tag":"li","lines":"179,183"}},{"content":"Beispiel (<code>spark-submit</code>):<pre data-lines=\"184,190\"><code class=\"language-bash\">spark-submit \\ \n\t--conf spark.executor.memory=6g \\\n\t--conf spark.executor.cores=2 \\\n\tmy_script.py\n</code></pre>","children":[],"payload":{"tag":"li","lines":"183,190"}},{"content":"Beispiel:<pre data-lines=\"191,200\"><code class=\"language-python\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\nspark = SparkSession.builder \\\n    .appName(<span class=\"hljs-string\">&quot;DynamicConfigExample&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.memory&quot;</span>, <span class=\"hljs-string\">&quot;6g&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.cores&quot;</span>, <span class=\"hljs-string\">&quot;2&quot;</span>) \\\n    .getOrCreate()\n</code></pre>","children":[],"payload":{"tag":"li","lines":"190,201"}}],"payload":{"tag":"h4","lines":"176,177"}}],"payload":{"tag":"h3","lines":"161,162"}},{"content":"Property Precedence (Priorit&#xe4;t von Spark-Konfigurationen)","children":[{"content":"<strong>Dynamische Konfiguration in der Anwendung</strong>","children":[{"content":"<code>SparkConf</code> oder <code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"206,207"}},{"content":"&#xdc;berschreibt <strong>alle anderen</strong> Konfigurationen","children":[],"payload":{"tag":"li","lines":"207,209"}}],"payload":{"tag":"h4","lines":"205,206"}},{"content":"<strong><code>spark-submit</code> oder <code>spark-shell</code> Argumente</strong>","children":[{"content":"<code>--conf</code> Optionen &#xfc;berschreiben Werte aus <code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"210,212"}}],"payload":{"tag":"h4","lines":"209,210"}},{"content":"<strong><code>spark-defaults.conf</code> (Globale Standardeinstellungen)</strong>","children":[{"content":"Wird verwendet, wenn keine spezifische <code>--conf</code> Option gesetzt wurde","children":[],"payload":{"tag":"li","lines":"213,215"}}],"payload":{"tag":"h4","lines":"212,213"}},{"content":"<strong>Environment Variables (<code>spark-env.sh</code>)</strong>","children":[{"content":"Setzt Umgebungsvariablen, z. B. Speicher- und CPU-Limits f&#xfc;r Worker","children":[],"payload":{"tag":"li","lines":"216,217"}},{"content":"Gilt f&#xfc;r die gesamte Umgebung, nicht nur f&#xfc;r eine einzelne Anwendung","children":[],"payload":{"tag":"li","lines":"217,219"}}],"payload":{"tag":"h4","lines":"215,216"}},{"content":"<strong>Spark-Standardwerte (Niedrigste Priorit&#xe4;t)</strong>","children":[{"content":"Falls nirgends definiert, nutzt Spark eigene Standardwerte","children":[],"payload":{"tag":"li","lines":"220,221"}},{"content":"Beispiel:","children":[{"content":"<code>spark.executor.memory</code>: Standard <strong>1 GB</strong>","children":[],"payload":{"tag":"li","lines":"222,223"}},{"content":"<code>spark.executor.cores</code>: Standard <strong>alle verf&#xfc;gbaren Kerne</strong>","children":[],"payload":{"tag":"li","lines":"223,225"}}],"payload":{"tag":"li","lines":"221,225"}}],"payload":{"tag":"h4","lines":"219,220"}}],"payload":{"tag":"h3","lines":"201,202"}}],"payload":{"tag":"h2","lines":"159,160"}},{"content":"Languages","children":[{"content":"Scala","children":[],"payload":{"tag":"li","lines":"226,227"}},{"content":"Java","children":[],"payload":{"tag":"li","lines":"227,228"}},{"content":"Python (PySpark)","children":[],"payload":{"tag":"li","lines":"228,229"}},{"content":"R (SparkR)","children":[],"payload":{"tag":"li","lines":"229,231"}}],"payload":{"tag":"h2","lines":"225,226"}},{"content":"Data Sources","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"232,233"}},{"content":"S3","children":[],"payload":{"tag":"li","lines":"233,234"}},{"content":"HBase","children":[],"payload":{"tag":"li","lines":"234,235"}},{"content":"Cassandra","children":[],"payload":{"tag":"li","lines":"235,236"}},{"content":"JDBC","children":[{"content":"JSON","children":[],"payload":{"tag":"li","lines":"237,238"}},{"content":"Parquet","children":[],"payload":{"tag":"li","lines":"238,239"}},{"content":"Avro","children":[],"payload":{"tag":"li","lines":"239,241"}}],"payload":{"tag":"li","lines":"236,241"}}],"payload":{"tag":"h2","lines":"231,232"}},{"content":"Operations","children":[{"content":"<strong>Transformations</strong> (Erzeugen neue RDDs, sind lazy)","children":[{"content":"<strong>Narrow Transformations</strong> (keine oder minimale Shuffles)","children":[{"content":"Map","children":[],"payload":{"tag":"li","lines":"244,245"}},{"content":"Filter","children":[],"payload":{"tag":"li","lines":"245,246"}},{"content":"FlatMap","children":[],"payload":{"tag":"li","lines":"246,247"}}],"payload":{"tag":"li","lines":"243,247"}},{"content":"<strong>Wide Transformations</strong> (erfordern Shuffling der Daten)","children":[{"content":"GroupByKey","children":[],"payload":{"tag":"li","lines":"248,249"}},{"content":"ReduceByKey","children":[],"payload":{"tag":"li","lines":"249,250"}},{"content":"Join","children":[],"payload":{"tag":"li","lines":"250,251"}}],"payload":{"tag":"li","lines":"247,251"}}],"payload":{"tag":"li","lines":"242,251"}},{"content":"<strong>Actions</strong> (L&#xf6;sen die tats&#xe4;chliche Berechnung aus)","children":[{"content":"Collect","children":[],"payload":{"tag":"li","lines":"252,253"}},{"content":"Count","children":[],"payload":{"tag":"li","lines":"253,254"}},{"content":"SaveAsTextFile","children":[],"payload":{"tag":"li","lines":"254,255"}},{"content":"Reduce","children":[],"payload":{"tag":"li","lines":"255,256"}},{"content":"Take","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"First","children":[],"payload":{"tag":"li","lines":"257,258"}},{"content":"Foreach","children":[],"payload":{"tag":"li","lines":"258,259"}},{"content":"CountByValue","children":[],"payload":{"tag":"li","lines":"259,261"}}],"payload":{"tag":"li","lines":"251,261"}}],"payload":{"tag":"h2","lines":"241,242"}},{"content":"Performance Optimization","children":[{"content":"<strong>Caching and Persistence</strong> (Speicherlevel: <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>)","children":[{"content":"H&#xe4;ufig genutzte Daten k&#xf6;nnen zwischengespeichert werden, um die Leistung zu steigern","children":[],"payload":{"tag":"li","lines":"263,264"}},{"content":"<strong>Persistieren</strong>: Wann und wie man Daten f&#xfc;r wiederholte Berechnungen speichert","children":[],"payload":{"tag":"li","lines":"264,265"}}],"payload":{"tag":"li","lines":"262,265"}},{"content":"<strong>Serialization</strong> (Kryo vs. Java)","children":[{"content":"<strong>Kryo</strong>: Schnellere und kleinere Serialisierung im Vergleich zu Java","children":[],"payload":{"tag":"li","lines":"266,267"}}],"payload":{"tag":"li","lines":"265,267"}},{"content":"<strong>Partitioning</strong> (Aufteilung der Daten f&#xfc;r bessere Parallelverarbeitung)","children":[{"content":"<strong>Types of Partitioning</strong>","children":[{"content":"<strong>Hash Partitioning</strong> (Standard bei <code>reduceByKey</code>, <code>groupBy</code>)","children":[],"payload":{"tag":"li","lines":"269,270"}},{"content":"<strong>Range Partitioning</strong> (Sortiert Daten in Bereiche, gut f&#xfc;r Skew-Probleme)","children":[],"payload":{"tag":"li","lines":"270,271"}},{"content":"<strong>Custom Partitioning</strong> (Eigene Logik durch <code>partitionBy</code>)","children":[],"payload":{"tag":"li","lines":"271,272"}}],"payload":{"tag":"li","lines":"268,272"}},{"content":"<strong>Optimizing Partitioning</strong>","children":[{"content":"<strong>Repartition vs. Coalesce</strong> (<code>repartition(n)</code> f&#xfc;r gleichm&#xe4;&#xdf;ige Verteilung, <code>coalesce(n)</code> f&#xfc;r Zusammenfassung ohne gro&#xdf;e Shuffle-Kosten)","children":[],"payload":{"tag":"li","lines":"273,274"}},{"content":"<strong>Data Locality</strong> (Partitionen nahe an den verarbeitenden Nodes halten)","children":[],"payload":{"tag":"li","lines":"274,275"}},{"content":"<strong>Avoid Small Files</strong> (Zu viele kleine Partitionen sind ineffizient)","children":[],"payload":{"tag":"li","lines":"275,276"}}],"payload":{"tag":"li","lines":"272,276"}}],"payload":{"tag":"li","lines":"267,276"}},{"content":"<strong>Tungsten Engine</strong> (Optimierung der Speicher- und CPU-Nutzung)","children":[{"content":"<strong>Bytecode Generation</strong> (Vermeidung von JVM-Overhead durch direkte Code-Erzeugung)","children":[],"payload":{"tag":"li","lines":"277,278"}},{"content":"<strong>Cache-aware computation</strong> (Effiziente Nutzung von CPU-Caches)","children":[],"payload":{"tag":"li","lines":"278,279"}},{"content":"<strong>Vectorized Processing</strong> (Batch-Verarbeitung von Daten f&#xfc;r bessere Performance)","children":[],"payload":{"tag":"li","lines":"279,280"}},{"content":"<strong>Binary Processing</strong> (Arbeiten mit bin&#xe4;ren Formaten statt Java-Objekten)","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"<strong>Memory Management</strong> (Vermeidung von Garbage Collection durch manuelle Speicherverwaltung)","children":[],"payload":{"tag":"li","lines":"281,282"}}],"payload":{"tag":"li","lines":"276,282"}},{"content":"<strong>Shuffle Operations</strong> (Datenbewegung zwischen Nodes, teuer in der Verarbeitung)","children":[{"content":"<strong>Why Shuffle Happens?</strong> (z. B. bei <code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>)","children":[],"payload":{"tag":"li","lines":"283,284"}},{"content":"<strong>Shuffle Write</strong> (Partitionierte Daten werden auf Festplatte/Netzwerk gespeichert)","children":[],"payload":{"tag":"li","lines":"284,285"}},{"content":"<strong>Shuffle Read</strong> (Andere Worker laden diese Daten zur Verarbeitung)","children":[],"payload":{"tag":"li","lines":"285,286"}},{"content":"<strong>Mitigation Strategies</strong>","children":[{"content":"<strong>Partitioning</strong> (z. B. <code>coalesce</code>, <code>repartition</code> zur Optimierung)","children":[],"payload":{"tag":"li","lines":"287,288"}},{"content":"<strong>ReduceByKey statt GroupByKey</strong> (Minimierung der Shuffle-Gr&#xf6;&#xdf;e)","children":[],"payload":{"tag":"li","lines":"288,289"}},{"content":"<strong>Broadcast Joins</strong> (Vermeidung von teuren Shuffles)","children":[],"payload":{"tag":"li","lines":"289,291"}}],"payload":{"tag":"li","lines":"286,291"}}],"payload":{"tag":"li","lines":"282,291"}}],"payload":{"tag":"h2","lines":"261,262"}},{"content":"Deployment Modes","children":[{"content":"<strong>Local Mode</strong>","children":[{"content":"Eignet sich f&#xfc;r Tests und kleine Datens&#xe4;tze, l&#xe4;uft auf einem einzigen Rechner.","children":[],"payload":{"tag":"li","lines":"293,294"}}],"payload":{"tag":"li","lines":"292,294"}},{"content":"<strong>Cluster Mode</strong>","children":[{"content":"Optimiert f&#xfc;r gro&#xdf;e Datenmengen und f&#xfc;hrt Jobs auf mehreren Maschinen aus.","children":[],"payload":{"tag":"li","lines":"295,296"}},{"content":"<strong>Cluster Manager Deployment</strong>:","children":[{"content":"<strong>YARN-client</strong>: Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks im Cluster laufen.","children":[],"payload":{"tag":"li","lines":"297,298"}},{"content":"<strong>YARN-cluster</strong>: Der Driver l&#xe4;uft im Cluster.","children":[],"payload":{"tag":"li","lines":"298,299"}},{"content":"<strong>Kubernetes</strong>: Verwendung von Kubernetes als Cluster Manager zur Verwaltung von Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"299,300"}}],"payload":{"tag":"li","lines":"296,300"}}],"payload":{"tag":"li","lines":"294,300"}},{"content":"<strong>Client Mode</strong>","children":[{"content":"Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks auf dem Cluster ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"301,303"}}],"payload":{"tag":"li","lines":"300,303"}}],"payload":{"tag":"h2","lines":"291,292"}},{"content":"Ecosystem Integration","children":[{"content":"<strong>Hadoop Ecosystem</strong>","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"305,306"}},{"content":"YARN","children":[],"payload":{"tag":"li","lines":"306,307"}},{"content":"Hive","children":[],"payload":{"tag":"li","lines":"307,308"}}],"payload":{"tag":"li","lines":"304,308"}},{"content":"<strong>Cloud Services (Spark Deployment)</strong>","children":[{"content":"<strong>AWS (EMR)</strong>: Skalierbare und kosteneffiziente L&#xf6;sung f&#xfc;r Spark auf AWS.","children":[],"payload":{"tag":"li","lines":"309,310"}},{"content":"<strong>Azure (Databricks)</strong>: Managed Spark-Plattform f&#xfc;r fortschrittliche Analysen und maschinelles Lernen.","children":[],"payload":{"tag":"li","lines":"310,311"}},{"content":"<strong>IBM Cloud</strong>: Spark-Integration in IBM Cloud Pak for Data, ideal f&#xfc;r Hybrid-Cloud-Umgebungen und AI-Integration.","children":[],"payload":{"tag":"li","lines":"311,312"}},{"content":"<strong>Azure HDInsight</strong>: Managed Big Data Plattform von Microsoft f&#xfc;r Spark und andere Big Data-Tools.","children":[],"payload":{"tag":"li","lines":"312,313"}},{"content":"<strong>GCP (DataProc)</strong>: Google Cloud&#x2019;s verwaltete Spark- und Hadoop-Dienste f&#xfc;r skalierbare Datenverarbeitung.","children":[],"payload":{"tag":"li","lines":"313,314"}}],"payload":{"tag":"li","lines":"308,314"}},{"content":"<strong>BI Tools</strong>","children":[{"content":"Tableau","children":[],"payload":{"tag":"li","lines":"315,316"}},{"content":"Power BI","children":[],"payload":{"tag":"li","lines":"316,318"}}],"payload":{"tag":"li","lines":"314,318"}}],"payload":{"tag":"h2","lines":"303,304"}},{"content":"Libraries and Extensions","children":[{"content":"SparkR","children":[],"payload":{"tag":"li","lines":"319,320"}},{"content":"PySpark","children":[],"payload":{"tag":"li","lines":"320,321"}},{"content":"GraphFrames","children":[],"payload":{"tag":"li","lines":"321,322"}},{"content":"Koalas","children":[],"payload":{"tag":"li","lines":"322,324"}}],"payload":{"tag":"h2","lines":"318,319"}},{"content":"Spark Cluster Fehlerbehebung","children":[{"content":"Monitoring &amp; Debugging","children":[{"content":"Spark UI","children":[{"content":"Web-Interface zur &#xdc;berwachung von Jobs und deren Performance","children":[],"payload":{"tag":"li","lines":"328,329"}}],"payload":{"tag":"h4","lines":"327,328"}},{"content":"Event Logs","children":[{"content":"Detaillierte Logs, die zur Fehleranalyse und Performanceoptimierung genutzt werden","children":[],"payload":{"tag":"li","lines":"330,331"}}],"payload":{"tag":"h4","lines":"329,330"}},{"content":"Metrics","children":[{"content":"<strong>Ganglia</strong>","children":[],"payload":{"tag":"li","lines":"332,333"}},{"content":"<strong>Graphite</strong>","children":[],"payload":{"tag":"li","lines":"333,334"}}],"payload":{"tag":"h4","lines":"331,332"}},{"content":"Logging","children":[{"content":"<strong>Log4j</strong>","children":[],"payload":{"tag":"li","lines":"335,337"}}],"payload":{"tag":"h4","lines":"334,335"}}],"payload":{"tag":"h3","lines":"326,327"}},{"content":"Benutzercode","children":[{"content":"Fehler","children":[{"content":"<strong>NullPointerException</strong> / <strong>IndexOutOfBoundsException</strong>","children":[],"payload":{"tag":"li","lines":"339,340"}},{"content":"<strong>ClassNotFoundException</strong> / <strong>NoSuchMethodError</strong>","children":[],"payload":{"tag":"li","lines":"340,341"}},{"content":"<strong>SerializationException</strong>","children":[],"payload":{"tag":"li","lines":"341,342"}}],"payload":{"tag":"h5","lines":"338,339"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Debugging &amp; Logging</strong> (Spark-Logs, Stacktrace pr&#xfc;fen)","children":[],"payload":{"tag":"li","lines":"343,344"}},{"content":"<strong>Richtige Abh&#xe4;ngigkeiten einbinden</strong> (<code>--jars</code>, <code>--packages</code>)","children":[],"payload":{"tag":"li","lines":"344,345"}},{"content":"<strong>RDD-Transformationen pr&#xfc;fen</strong> (Lazy Evaluation beachten)","children":[],"payload":{"tag":"li","lines":"345,346"}},{"content":"<strong>Korrekte Serialisierung sicherstellen</strong> (<code>KryoSerializer</code> verwenden)","children":[],"payload":{"tag":"li","lines":"346,348"}}],"payload":{"tag":"h5","lines":"342,343"}}],"payload":{"tag":"h3","lines":"337,338"}},{"content":"System- &amp; Anwendungskonfiguration","children":[{"content":"Fehler","children":[{"content":"<strong>Inkompatible Spark- / Hadoop-Version</strong>","children":[],"payload":{"tag":"li","lines":"350,351"}},{"content":"<strong>Fehlende Umgebungsvariablen</strong> (<code>SPARK_HOME</code>, <code>HADOOP_CONF_DIR</code>)","children":[],"payload":{"tag":"li","lines":"351,352"}},{"content":"<strong>Unsachgem&#xe4;&#xdf;e Speicherverwaltung</strong>","children":[],"payload":{"tag":"li","lines":"352,353"}}],"payload":{"tag":"h5","lines":"349,350"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Versionen pr&#xfc;fen</strong> (<code>spark-submit --version</code>, <code>hadoop version</code>)","children":[],"payload":{"tag":"li","lines":"354,355"}},{"content":"<strong>Umgebungsvariablen setzen</strong>","children":[],"payload":{"tag":"li","lines":"355,356"}},{"content":"<strong>Speicherlimits optimieren</strong> (<code>spark.driver.memory</code>, <code>spark.executor.memory</code>)","children":[],"payload":{"tag":"li","lines":"356,358"}}],"payload":{"tag":"h5","lines":"353,354"}}],"payload":{"tag":"h3","lines":"348,349"}},{"content":"Fehlende oder falsche Anwendungsversion","children":[{"content":"Fehler","children":[{"content":"<strong>Nicht gefundene Bibliotheken</strong> (<code>NoClassDefFoundError</code>)","children":[],"payload":{"tag":"li","lines":"360,361"}},{"content":"<strong>Inkompatible JAR-Dateien</strong>","children":[],"payload":{"tag":"li","lines":"361,362"}}],"payload":{"tag":"h5","lines":"359,360"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Richtige Abh&#xe4;ngigkeiten sicherstellen</strong> (<code>--jars</code>, <code>--files</code>)","children":[],"payload":{"tag":"li","lines":"363,364"}},{"content":"<strong>Versionen von Bibliotheken abgleichen</strong>","children":[],"payload":{"tag":"li","lines":"364,366"}}],"payload":{"tag":"h5","lines":"362,363"}}],"payload":{"tag":"h3","lines":"358,359"}},{"content":"Falsche Ressourcenzuweisung","children":[{"content":"Fehler","children":[{"content":"<strong>OutOfMemoryError</strong>","children":[],"payload":{"tag":"li","lines":"368,369"}},{"content":"<strong>Executor Lost</strong>","children":[],"payload":{"tag":"li","lines":"369,370"}},{"content":"<strong>Task Timeout</strong>","children":[],"payload":{"tag":"li","lines":"370,371"}}],"payload":{"tag":"h5","lines":"367,368"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Speicher &amp; Kerne anpassen</strong> (<code>spark.executor.memory</code>, <code>spark.executor.cores</code>)","children":[],"payload":{"tag":"li","lines":"372,373"}},{"content":"<strong>Dynamische Allokation aktivieren</strong> (<code>spark.dynamicAllocation.enabled=true</code>)","children":[],"payload":{"tag":"li","lines":"373,374"}},{"content":"<strong>Partitionierung optimieren</strong> (<code>repartition()</code>, <code>coalesce()</code>)","children":[],"payload":{"tag":"li","lines":"374,376"}}],"payload":{"tag":"h5","lines":"371,372"}}],"payload":{"tag":"h3","lines":"366,367"}},{"content":"Netzwerkprobleme","children":[{"content":"Fehler","children":[{"content":"<strong>Timeouts bei Daten&#xfc;bertragung</strong>","children":[],"payload":{"tag":"li","lines":"378,379"}},{"content":"<strong>Verlorene Cluster-Knoten</strong>","children":[],"payload":{"tag":"li","lines":"379,380"}},{"content":"<strong>Verbindungsprobleme zwischen Master &amp; Worker</strong>","children":[],"payload":{"tag":"li","lines":"380,381"}}],"payload":{"tag":"h5","lines":"377,378"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Netzwerkkonfiguration pr&#xfc;fen</strong> (<code>spark.network.timeout</code>)","children":[],"payload":{"tag":"li","lines":"382,383"}},{"content":"<strong>Heartbeat-Intervalle anpassen</strong> (<code>spark.executor.heartbeatInterval</code>)","children":[],"payload":{"tag":"li","lines":"383,384"}},{"content":"<strong>Fehlertoleranzmechanismen nutzen</strong> (<code>spark.task.maxFailures</code>)","children":[],"payload":{"tag":"li","lines":"384,387"}}],"payload":{"tag":"h5","lines":"381,382"}}],"payload":{"tag":"h3","lines":"376,377"}}],"payload":{"tag":"h2","lines":"324,325"}}],"payload":{"tag":"h1","lines":"8,9"}},{"colorFreezeLevel":2,"maxWidth":300,"activeNode":{}})</script>
</body>
</html>
