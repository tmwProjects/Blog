<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Apache Spark","children":[{"content":"Core Components","children":[{"content":"<strong>Spark Core</strong>","children":[{"content":"Grundlegende Architektur von Spark","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Ressourcenverwaltung und Scheduling","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"Fehlerbehandlung und Wiederholbarkeit von Jobs","children":[],"payload":{"tag":"li","lines":"16,17"}}],"payload":{"tag":"li","lines":"13,17"}},{"content":"<strong>RDD (Resilient Distributed Dataset)</strong>","children":[{"content":"Unterschiede zu DataFrames und Datasets","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Transformationen und Aktionen","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Vorteile der Fehlertoleranz und Parallelit&#xe4;t","children":[],"payload":{"tag":"li","lines":"20,21"}}],"payload":{"tag":"li","lines":"17,21"}},{"content":"<strong>DAG (Directed Acyclic Graph)</strong>","children":[{"content":"Steuerung der Transformationen und deren Reihenfolge","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"Fehlerbehandlung und Neuberechnung von fehlgeschlagenen Tasks","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Einfluss auf die Performance und Optimierung der Berechnungen","children":[],"payload":{"tag":"li","lines":"24,25"}}],"payload":{"tag":"li","lines":"21,25"}},{"content":"<strong>Shared Variables</strong>","children":[{"content":"Einsatz von Broadcast Variables zur Minimierung von Netzwerkverkehr","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"Einsatz von Accumulators f&#xfc;r Z&#xe4;hleroperationen in verteilten Berechnungen","children":[],"payload":{"tag":"li","lines":"27,28"}}],"payload":{"tag":"li","lines":"25,28"}},{"content":"<strong>Spark SQL</strong>","children":[{"content":"DataFrames","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"Datasets","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"<strong>Catalyst Optimizer</strong>","children":[{"content":"Logical Plan Optimization","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Predicate Pushdown","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Join Reordering","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Cost-Based Optimization (CBO)","children":[],"payload":{"tag":"li","lines":"35,36"}},{"content":"Code Generation (Tungsten Engine)","children":[],"payload":{"tag":"li","lines":"36,37"}}],"payload":{"tag":"li","lines":"31,37"}}],"payload":{"tag":"li","lines":"28,37"}},{"content":"<strong>Spark Streaming</strong>","children":[{"content":"DStream (Discretized Stream)","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"Structured Streaming","children":[],"payload":{"tag":"li","lines":"39,40"}}],"payload":{"tag":"li","lines":"37,40"}},{"content":"<strong>MLlib (Machine Learning Library)</strong>","children":[{"content":"Algorithms","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"Pipelines","children":[],"payload":{"tag":"li","lines":"42,43"}},{"content":"Feature Transformation","children":[],"payload":{"tag":"li","lines":"43,44"}}],"payload":{"tag":"li","lines":"40,44"}},{"content":"<strong>GraphX</strong>","children":[{"content":"Graph Processing","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"Pregel API","children":[],"payload":{"tag":"li","lines":"46,48"}}],"payload":{"tag":"li","lines":"44,48"}}],"payload":{"tag":"h2","lines":"12,13"}},{"content":"Cluster Architecture","children":[{"content":"\n<p data-lines=\"49,50\"><strong>Master</strong></p>","children":[{"content":"Verantwortlich f&#xfc;r die Verwaltung von Ressourcen und die Verteilung von Tasks","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Entscheidet, auf welchem Worker-Node Tasks ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Verbindet und f&#xfc;gt neue Workers hinzu.","children":[],"payload":{"tag":"li","lines":"52,53"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"54,57\"><code data-lines=\"54,57\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-master.sh</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"54,57"}}],"payload":{"tag":"li","lines":"53,57"}}],"payload":{"tag":"li","lines":"49,57"}},{"content":"\n<p data-lines=\"57,58\"><strong>Worker</strong></p>","children":[{"content":"F&#xfc;hren die tats&#xe4;chliche Verarbeitung der Daten aus.","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Halten den Executor, der die Tasks ausf&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Werden auf Cluster-Nodes ausgef&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"60,61"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"62,65\"><code data-lines=\"62,65\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-slave.sh spark://&lt;master-spark-URL&gt;:7077</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"62,65"}}],"payload":{"tag":"li","lines":"61,65"}}],"payload":{"tag":"li","lines":"57,65"}},{"content":"\n<p data-lines=\"65,66\"><strong>Driver</strong></p>","children":[{"content":"Verwaltet den Spark-Job, koordiniert die Arbeit der Cluster-Ressourcen","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"Stellt eine Verbindung zum Master her und verteilt Tasks","children":[],"payload":{"tag":"li","lines":"67,68"}}],"payload":{"tag":"li","lines":"65,68"}},{"content":"\n<p data-lines=\"68,69\"><strong>Executor</strong></p>","children":[{"content":"F&#xfc;hren Tasks auf Worker-Nodes aus und speichern Ergebnisse","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"Jeder Executor f&#xfc;hrt Aufgaben f&#xfc;r den zugewiesenen Task aus und speichert Daten lokal","children":[],"payload":{"tag":"li","lines":"70,71"}}],"payload":{"tag":"li","lines":"68,71"}},{"content":"\n<p data-lines=\"71,72\"><strong>Cluster Manager</strong></p>","children":[{"content":"Verwalten des Ressourcenmanagements im Cluster","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"Der Cluster Manager &#xfc;bernimmt das Starten von Executorn und das Zuweisen von Ressourcen","children":[{"content":"\n<p data-lines=\"74,75\">Standalone</p>","children":[{"content":"Es sid keine zus&#xe4;tzlichen Abh&#xe4;ngigkeiten notwendig f&#xfc;r Deployment.","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"Der schnellste Weg einen Cluster auszusetzen und laufen zu lassen.","children":[],"payload":{"tag":"li","lines":"76,77"}}],"payload":{"tag":"li","lines":"74,77"}},{"content":"\n<p data-lines=\"77,78\">Hadoop YARN</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"YARN-Cluster haben ihre eigenen Abh&#xe4;ngigkeiten.","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"Komplexer f&#xfc;r Deployments.","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"Unterst&#xfc;tzt weitere zus&#xe4;tzliche Frameworks:","children":[{"content":"Apache Spark","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Apache MapReduce","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"Apache Tez","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"Apache Flink","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Apache HBase","children":[],"payload":{"tag":"li","lines":"86,87"}},{"content":"Apache Storm","children":[],"payload":{"tag":"li","lines":"87,88"}},{"content":"Apache Drill","children":[],"payload":{"tag":"li","lines":"88,89"}}],"payload":{"tag":"li","lines":"81,89"}}],"payload":{"tag":"li","lines":"77,89"}},{"content":"\n<p data-lines=\"89,90\">Apache Mesos</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"90,91"}},{"content":"M&#xf6;glichkeit f&#xfc;r dynamische Partitionierungen zw. Spark und anderen Big-Data-Frameworks.","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"Skalierbar zwischen mehreren Spark-Instanzen.","children":[],"payload":{"tag":"li","lines":"92,93"}}],"payload":{"tag":"li","lines":"89,93"}},{"content":"\n<p data-lines=\"93,94\">Kubernetes</p>","children":[{"content":"Automatisierte Deployments.","children":[],"payload":{"tag":"li","lines":"94,95"}},{"content":"Portierbare Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"Skaliert des Cluster.","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Vereinfachtes Abh&#xe4;ngigkeits-Management.","children":[],"payload":{"tag":"li","lines":"97,98"}}],"payload":{"tag":"li","lines":"93,98"}},{"content":"\n<p data-lines=\"98,99\"><strong>Spark-Shell</strong></p>","children":[{"content":"Interaktive Shell f&#xfc;r das schnelle Testen von Spark-Funktionen","children":[{"content":"Wird oft genutzt, um Spark-APIs zu lernen und schnelle Experimente durchzuf&#xfc;hren","children":[],"payload":{"tag":"li","lines":"100,101"}}],"payload":{"tag":"li","lines":"99,101"}},{"content":"Verf&#xfc;gbar f&#xfc;r Python und Scala.","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"Sobald Spark-Shell gestartet wurde, kann direkt mit Daten gearbeitet werden.","children":[],"payload":{"tag":"li","lines":"102,103"}},{"content":"Starte:<pre data-lines=\"104,107\"><code data-lines=\"104,107\">./bin/pyspark\n</code></pre>","children":[],"payload":{"tag":"li","lines":"103,108"}}],"payload":{"tag":"li","lines":"98,108"}},{"content":"\n<p data-lines=\"108,109\">Local Mode.</p>","children":[{"content":"L&#xe4;uft ohne Cluster-Manager","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"Nutzt nur die lokale CPU und den Arbeitsspeicher der Maschine","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Kann in verschiedenen Konfigurationen betrieben werden","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"Eignet sich gut f&#xfc;r Entwicklung, Debugging und kleine Datenmengen.","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"Starte und konfiguriere:","children":[{"content":"\n<pre data-lines=\"114,119\"><code data-lines=\"114,119\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./bin/spark-submit \\\n    --master <span class=\"hljs-built_in\">local</span>[#] \\\n    &lt;additional configuration&gt;</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"114,120"}},{"content":"\n<p data-lines=\"120,121\"><strong><code>spark-submit</code></strong></p>","children":[{"content":"\n<p data-lines=\"121,122\">Kommandozeilen-Tool, mit dem Apache Spark-Anwendungen gestartet werden. Es erm&#xf6;glicht die Ausf&#xfc;hrung von Spark-Programmen in verschiedenen Modi (lokal oder auf einem Cluster) und mit verschiedenen Ressourcen-Konfigurationen.</p>","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"\n<p data-lines=\"122,123\">Das spark-submit-Skript liest zus&#xe4;tzliche spezifische Konfigurationen in <code>/conf/spark-defaults.conf</code></p>","children":[],"payload":{"tag":"li","lines":"122,123"}},{"content":"\n<p data-lines=\"123,124\">Stellt ggf. eine Verbindung zum zum Cluster Manager her.</p>","children":[],"payload":{"tag":"li","lines":"123,124"}},{"content":"\n<p data-lines=\"124,126\">Alle Anwendungsdateien (einschlie&#xdf;lich JARs oder Python-Dateien) m&#xfc;ssen angegeben werden, damit sie<br>\ndas Treiberprogramm starten und Dateien verteilen k&#xf6;nnen, die im Cluster ausgef&#xfc;hrt werden sollen.</p>","children":[],"payload":{"tag":"li","lines":"124,126"}},{"content":"\n<pre data-lines=\"126,135\"><code data-lines=\"126,135\"> spark-submit \\\n <span class=\"hljs-attr\">--master</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--deploy-mode</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--num-executors</span> &lt;anzahl&gt; \\\n <span class=\"hljs-attr\">--executor-memory</span> &lt;speicher&gt; \\\n <span class=\"hljs-attr\">--executor-cores</span> &lt;kerne&gt; \\\n mein_spark_programm<span class=\"hljs-selector-class\">.py</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"126,136"}},{"content":"\n<p data-lines=\"136,137\"><strong>Wichtige Parameter von `spark-submit</strong>:</p>","children":[{"content":"<strong><code>--master</code></strong>: Definiert, wo die Spark-App l&#xe4;uft (<code>local</code>, <code>yarn</code>, <code>kubernetes</code>, <code>mesos</code>)","children":[{"content":"Beispiel: <code>--master yarn</code>","children":[],"payload":{"tag":"li","lines":"138,139"}}],"payload":{"tag":"li","lines":"137,139"}},{"content":"<strong><code>--deploy-mode</code></strong>: <code>client</code> (Driver l&#xe4;uft lokal) oder <code>cluster</code> (Driver l&#xe4;uft im Cluster)","children":[{"content":"Beispiel: <code>--deploy-mode cluster</code>","children":[],"payload":{"tag":"li","lines":"140,141"}}],"payload":{"tag":"li","lines":"139,141"}},{"content":"<strong><code>--num-executors</code></strong>: Anzahl der Executors im Cluster","children":[{"content":"Beispiel: <code>--num-executors 5</code>","children":[],"payload":{"tag":"li","lines":"142,143"}}],"payload":{"tag":"li","lines":"141,143"}},{"content":"<strong><code>--executor-memory</code></strong>: Speicher pro Executor","children":[{"content":"Beispiel: <code>--executor-memory 4G</code>","children":[],"payload":{"tag":"li","lines":"144,145"}}],"payload":{"tag":"li","lines":"143,145"}},{"content":"<strong><code>--executor-cores</code></strong>: CPU-Kerne pro Executor","children":[{"content":"Beispiel: <code>--executor-cores 2</code>","children":[],"payload":{"tag":"li","lines":"146,147"}}],"payload":{"tag":"li","lines":"145,147"}},{"content":"<strong><code>--driver-memory</code></strong>: Speicher f&#xfc;r den Driver","children":[{"content":"Beispiel: <code>--driver-memory 2G</code>","children":[],"payload":{"tag":"li","lines":"148,149"}}],"payload":{"tag":"li","lines":"147,149"}},{"content":"<strong><code>--conf</code></strong>: Zus&#xe4;tzliche Konfigurationen setzen","children":[{"content":"Beispiel: <code>--conf spark.sql.shuffle.partitions=50</code>","children":[],"payload":{"tag":"li","lines":"150,151"}}],"payload":{"tag":"li","lines":"149,151"}},{"content":"Wildcard (*) nutzen f&#xfc;r ALLE CPU-Kerne. Ansonsten Anzahl (#) angeben.","children":[],"payload":{"tag":"li","lines":"151,154"}}],"payload":{"tag":"li","lines":"136,154"}}],"payload":{"tag":"li","lines":"120,154"}}],"payload":{"tag":"li","lines":"113,154"}}],"payload":{"tag":"li","lines":"108,154"}}],"payload":{"tag":"li","lines":"73,154"}}],"payload":{"tag":"li","lines":"71,154"}},{"content":"\n<p data-lines=\"154,155\"><strong>Task</strong></p>","children":[{"content":"Kleinste Ausf&#xfc;hrungseinheit, die von einem Executor verarbeitet wird","children":[],"payload":{"tag":"li","lines":"155,156"}},{"content":"Besteht aus einer Transformation und/oder einer Aktion","children":[],"payload":{"tag":"li","lines":"156,157"}}],"payload":{"tag":"li","lines":"154,157"}},{"content":"\n<p data-lines=\"157,158\"><strong>Job</strong></p>","children":[{"content":"Besteht aus mehreren Tasks, die von einem Driver koordiniert werden","children":[],"payload":{"tag":"li","lines":"158,159"}},{"content":"Jeder Job besteht aus einem DAG von Tasks, die auf verschiedenen Nodes ausgef&#xfc;hrt werden","children":[],"payload":{"tag":"li","lines":"159,161"}}],"payload":{"tag":"li","lines":"157,161"}}],"payload":{"tag":"h2","lines":"48,49"}},{"content":"Spark Konfigurationen","children":[{"content":"Statische vs. Dynamische Konfiguration","children":[{"content":"<strong>Statische Konfiguration</strong>","children":[{"content":"Wird <strong>vor dem Start</strong> von Spark gesetzt","children":[],"payload":{"tag":"li","lines":"166,167"}},{"content":"&#xc4;nderungen erfordern einen <strong>Neustart</strong>","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"Typische Orte:","children":[{"content":"<code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"169,170"}},{"content":"<code>spark-env.sh</code>","children":[],"payload":{"tag":"li","lines":"170,171"}},{"content":"<code>log4j.properties</code>","children":[],"payload":{"tag":"li","lines":"171,172"}}],"payload":{"tag":"li","lines":"168,172"}},{"content":"Beispiel (<code>spark-defaults.conf</code>):<pre data-lines=\"173,177\"><code data-lines=\"173,177\">spark.executor.memory 4g\nspark.driver.memory 2g\n</code></pre>","children":[],"payload":{"tag":"li","lines":"172,178"}}],"payload":{"tag":"h4","lines":"165,166"}},{"content":"<strong>Dynamische Konfiguration</strong>","children":[{"content":"Kann <strong>zur Laufzeit</strong> ge&#xe4;ndert werden","children":[],"payload":{"tag":"li","lines":"179,180"}},{"content":"Keine Neustarts erforderlich","children":[],"payload":{"tag":"li","lines":"180,181"}},{"content":"Typische Methoden:","children":[{"content":"<code>spark-submit --conf</code>","children":[],"payload":{"tag":"li","lines":"182,183"}},{"content":"<code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"183,184"}},{"content":"Dynamische Ressourcenallokation (<code>spark.dynamicAllocation.enabled</code>)","children":[],"payload":{"tag":"li","lines":"184,185"}}],"payload":{"tag":"li","lines":"181,185"}},{"content":"Beispiel (<code>spark-submit</code>):<pre data-lines=\"186,192\"><code class=\"language-bash\">spark-submit \\ \n\t--conf spark.executor.memory=6g \\\n\t--conf spark.executor.cores=2 \\\n\tmy_script.py\n</code></pre>","children":[],"payload":{"tag":"li","lines":"185,192"}},{"content":"Beispiel:<pre data-lines=\"193,202\"><code class=\"language-python\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\nspark = SparkSession.builder \\\n    .appName(<span class=\"hljs-string\">&quot;DynamicConfigExample&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.memory&quot;</span>, <span class=\"hljs-string\">&quot;6g&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.cores&quot;</span>, <span class=\"hljs-string\">&quot;2&quot;</span>) \\\n    .getOrCreate()\n</code></pre>","children":[],"payload":{"tag":"li","lines":"192,203"}}],"payload":{"tag":"h4","lines":"178,179"}}],"payload":{"tag":"h3","lines":"163,164"}},{"content":"Property Precedence (Priorit&#xe4;t von Spark-Konfigurationen)","children":[{"content":"<strong>Dynamische Konfiguration in der Anwendung</strong>","children":[{"content":"<code>SparkConf</code> oder <code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"&#xdc;berschreibt <strong>alle anderen</strong> Konfigurationen","children":[],"payload":{"tag":"li","lines":"209,211"}}],"payload":{"tag":"h4","lines":"207,208"}},{"content":"<strong><code>spark-submit</code> oder <code>spark-shell</code> Argumente</strong>","children":[{"content":"<code>--conf</code> Optionen &#xfc;berschreiben Werte aus <code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"212,214"}}],"payload":{"tag":"h4","lines":"211,212"}},{"content":"<strong><code>spark-defaults.conf</code> (Globale Standardeinstellungen)</strong>","children":[{"content":"Wird verwendet, wenn keine spezifische <code>--conf</code> Option gesetzt wurde","children":[],"payload":{"tag":"li","lines":"215,217"}}],"payload":{"tag":"h4","lines":"214,215"}},{"content":"<strong>Environment Variables (<code>spark-env.sh</code>)</strong>","children":[{"content":"Setzt Umgebungsvariablen, z. B. Speicher- und CPU-Limits f&#xfc;r Worker","children":[],"payload":{"tag":"li","lines":"218,219"}},{"content":"Gilt f&#xfc;r die gesamte Umgebung, nicht nur f&#xfc;r eine einzelne Anwendung","children":[],"payload":{"tag":"li","lines":"219,221"}}],"payload":{"tag":"h4","lines":"217,218"}},{"content":"<strong>Spark-Standardwerte (Niedrigste Priorit&#xe4;t)</strong>","children":[{"content":"Falls nirgends definiert, nutzt Spark eigene Standardwerte","children":[],"payload":{"tag":"li","lines":"222,223"}},{"content":"Beispiel:","children":[{"content":"<code>spark.executor.memory</code>: Standard <strong>1 GB</strong>","children":[],"payload":{"tag":"li","lines":"224,225"}},{"content":"<code>spark.executor.cores</code>: Standard <strong>alle verf&#xfc;gbaren Kerne</strong>","children":[],"payload":{"tag":"li","lines":"225,227"}}],"payload":{"tag":"li","lines":"223,227"}}],"payload":{"tag":"h4","lines":"221,222"}}],"payload":{"tag":"h3","lines":"203,204"}}],"payload":{"tag":"h2","lines":"161,162"}},{"content":"Languages","children":[{"content":"Scala","children":[],"payload":{"tag":"li","lines":"228,229"}},{"content":"Java","children":[],"payload":{"tag":"li","lines":"229,230"}},{"content":"Python (PySpark)","children":[],"payload":{"tag":"li","lines":"230,231"}},{"content":"R (SparkR)","children":[],"payload":{"tag":"li","lines":"231,233"}}],"payload":{"tag":"h2","lines":"227,228"}},{"content":"Data Sources","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"234,235"}},{"content":"S3","children":[],"payload":{"tag":"li","lines":"235,236"}},{"content":"HBase","children":[],"payload":{"tag":"li","lines":"236,237"}},{"content":"Cassandra","children":[],"payload":{"tag":"li","lines":"237,238"}},{"content":"JDBC","children":[{"content":"JSON","children":[],"payload":{"tag":"li","lines":"239,240"}},{"content":"Parquet","children":[],"payload":{"tag":"li","lines":"240,241"}},{"content":"Avro","children":[],"payload":{"tag":"li","lines":"241,243"}}],"payload":{"tag":"li","lines":"238,243"}}],"payload":{"tag":"h2","lines":"233,234"}},{"content":"Operations","children":[{"content":"<strong>Transformations</strong> (Erzeugen neue RDDs, sind lazy)","children":[{"content":"<strong>Narrow Transformations</strong> (keine oder minimale Shuffles)","children":[{"content":"Map","children":[],"payload":{"tag":"li","lines":"246,247"}},{"content":"Filter","children":[],"payload":{"tag":"li","lines":"247,248"}},{"content":"FlatMap","children":[],"payload":{"tag":"li","lines":"248,249"}}],"payload":{"tag":"li","lines":"245,249"}},{"content":"<strong>Wide Transformations</strong> (erfordern Shuffling der Daten)","children":[{"content":"GroupByKey","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"ReduceByKey","children":[],"payload":{"tag":"li","lines":"251,252"}},{"content":"Join","children":[],"payload":{"tag":"li","lines":"252,253"}}],"payload":{"tag":"li","lines":"249,253"}}],"payload":{"tag":"li","lines":"244,253"}},{"content":"<strong>Actions</strong> (L&#xf6;sen die tats&#xe4;chliche Berechnung aus)","children":[{"content":"Collect","children":[],"payload":{"tag":"li","lines":"254,255"}},{"content":"Count","children":[],"payload":{"tag":"li","lines":"255,256"}},{"content":"SaveAsTextFile","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"Reduce","children":[],"payload":{"tag":"li","lines":"257,258"}},{"content":"Take","children":[],"payload":{"tag":"li","lines":"258,259"}},{"content":"First","children":[],"payload":{"tag":"li","lines":"259,260"}},{"content":"Foreach","children":[],"payload":{"tag":"li","lines":"260,261"}},{"content":"CountByValue","children":[],"payload":{"tag":"li","lines":"261,263"}}],"payload":{"tag":"li","lines":"253,263"}}],"payload":{"tag":"h2","lines":"243,244"}},{"content":"Performance Optimization","children":[{"content":"<strong>Caching and Persistence</strong> (Speicherlevel: <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>)","children":[{"content":"H&#xe4;ufig genutzte Daten k&#xf6;nnen zwischengespeichert werden, um die Leistung zu steigern","children":[],"payload":{"tag":"li","lines":"265,266"}},{"content":"<strong>Persistieren</strong>: Wann und wie man Daten f&#xfc;r wiederholte Berechnungen speichert","children":[],"payload":{"tag":"li","lines":"266,267"}}],"payload":{"tag":"li","lines":"264,267"}},{"content":"<strong>Serialization</strong> (Kryo vs. Java)","children":[{"content":"<strong>Kryo</strong>: Schnellere und kleinere Serialisierung im Vergleich zu Java","children":[],"payload":{"tag":"li","lines":"268,269"}}],"payload":{"tag":"li","lines":"267,269"}},{"content":"<strong>Partitioning</strong> (Aufteilung der Daten f&#xfc;r bessere Parallelverarbeitung)","children":[{"content":"<strong>Types of Partitioning</strong>","children":[{"content":"<strong>Hash Partitioning</strong> (Standard bei <code>reduceByKey</code>, <code>groupBy</code>)","children":[],"payload":{"tag":"li","lines":"271,272"}},{"content":"<strong>Range Partitioning</strong> (Sortiert Daten in Bereiche, gut f&#xfc;r Skew-Probleme)","children":[],"payload":{"tag":"li","lines":"272,273"}},{"content":"<strong>Custom Partitioning</strong> (Eigene Logik durch <code>partitionBy</code>)","children":[],"payload":{"tag":"li","lines":"273,274"}}],"payload":{"tag":"li","lines":"270,274"}},{"content":"<strong>Optimizing Partitioning</strong>","children":[{"content":"<strong>Repartition vs. Coalesce</strong> (<code>repartition(n)</code> f&#xfc;r gleichm&#xe4;&#xdf;ige Verteilung, <code>coalesce(n)</code> f&#xfc;r Zusammenfassung ohne gro&#xdf;e Shuffle-Kosten)","children":[],"payload":{"tag":"li","lines":"275,276"}},{"content":"<strong>Data Locality</strong> (Partitionen nahe an den verarbeitenden Nodes halten)","children":[],"payload":{"tag":"li","lines":"276,277"}},{"content":"<strong>Avoid Small Files</strong> (Zu viele kleine Partitionen sind ineffizient)","children":[],"payload":{"tag":"li","lines":"277,278"}}],"payload":{"tag":"li","lines":"274,278"}}],"payload":{"tag":"li","lines":"269,278"}},{"content":"<strong>Tungsten Engine</strong> (Optimierung der Speicher- und CPU-Nutzung)","children":[{"content":"<strong>Bytecode Generation</strong> (Vermeidung von JVM-Overhead durch direkte Code-Erzeugung)","children":[],"payload":{"tag":"li","lines":"279,280"}},{"content":"<strong>Cache-aware computation</strong> (Effiziente Nutzung von CPU-Caches)","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"<strong>Vectorized Processing</strong> (Batch-Verarbeitung von Daten f&#xfc;r bessere Performance)","children":[],"payload":{"tag":"li","lines":"281,282"}},{"content":"<strong>Binary Processing</strong> (Arbeiten mit bin&#xe4;ren Formaten statt Java-Objekten)","children":[],"payload":{"tag":"li","lines":"282,283"}},{"content":"<strong>Memory Management</strong> (Vermeidung von Garbage Collection durch manuelle Speicherverwaltung)","children":[],"payload":{"tag":"li","lines":"283,284"}}],"payload":{"tag":"li","lines":"278,284"}},{"content":"<strong>Shuffle Operations</strong> (Datenbewegung zwischen Nodes, teuer in der Verarbeitung)","children":[{"content":"<strong>Why Shuffle Happens?</strong> (z. B. bei <code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>)","children":[],"payload":{"tag":"li","lines":"285,286"}},{"content":"<strong>Shuffle Write</strong> (Partitionierte Daten werden auf Festplatte/Netzwerk gespeichert)","children":[],"payload":{"tag":"li","lines":"286,287"}},{"content":"<strong>Shuffle Read</strong> (Andere Worker laden diese Daten zur Verarbeitung)","children":[],"payload":{"tag":"li","lines":"287,288"}},{"content":"<strong>Mitigation Strategies</strong>","children":[{"content":"<strong>Partitioning</strong> (z. B. <code>coalesce</code>, <code>repartition</code> zur Optimierung)","children":[],"payload":{"tag":"li","lines":"289,290"}},{"content":"<strong>ReduceByKey statt GroupByKey</strong> (Minimierung der Shuffle-Gr&#xf6;&#xdf;e)","children":[],"payload":{"tag":"li","lines":"290,291"}},{"content":"<strong>Broadcast Joins</strong> (Vermeidung von teuren Shuffles)","children":[],"payload":{"tag":"li","lines":"291,293"}}],"payload":{"tag":"li","lines":"288,293"}}],"payload":{"tag":"li","lines":"284,293"}}],"payload":{"tag":"h2","lines":"263,264"}},{"content":"Deployment Modes","children":[{"content":"<strong>Local Mode</strong>","children":[{"content":"Eignet sich f&#xfc;r Tests und kleine Datens&#xe4;tze, l&#xe4;uft auf einem einzigen Rechner.","children":[],"payload":{"tag":"li","lines":"295,296"}}],"payload":{"tag":"li","lines":"294,296"}},{"content":"<strong>Cluster Mode</strong>","children":[{"content":"Optimiert f&#xfc;r gro&#xdf;e Datenmengen und f&#xfc;hrt Jobs auf mehreren Maschinen aus.","children":[],"payload":{"tag":"li","lines":"297,298"}},{"content":"<strong>Cluster Manager Deployment</strong>:","children":[{"content":"<strong>YARN-client</strong>: Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks im Cluster laufen.","children":[],"payload":{"tag":"li","lines":"299,300"}},{"content":"<strong>YARN-cluster</strong>: Der Driver l&#xe4;uft im Cluster.","children":[],"payload":{"tag":"li","lines":"300,301"}},{"content":"<strong>Kubernetes</strong>: Verwendung von Kubernetes als Cluster Manager zur Verwaltung von Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"301,302"}}],"payload":{"tag":"li","lines":"298,302"}}],"payload":{"tag":"li","lines":"296,302"}},{"content":"<strong>Client Mode</strong>","children":[{"content":"Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks auf dem Cluster ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"303,305"}}],"payload":{"tag":"li","lines":"302,305"}}],"payload":{"tag":"h2","lines":"293,294"}},{"content":"Ecosystem Integration","children":[{"content":"<strong>Hadoop Ecosystem</strong>","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"307,308"}},{"content":"YARN","children":[],"payload":{"tag":"li","lines":"308,309"}},{"content":"Hive","children":[],"payload":{"tag":"li","lines":"309,310"}}],"payload":{"tag":"li","lines":"306,310"}},{"content":"<strong>Cloud Services (Spark Deployment)</strong>","children":[{"content":"<strong>AWS (EMR)</strong>: Skalierbare und kosteneffiziente L&#xf6;sung f&#xfc;r Spark auf AWS.","children":[],"payload":{"tag":"li","lines":"311,312"}},{"content":"<strong>Azure (Databricks)</strong>: Managed Spark-Plattform f&#xfc;r fortschrittliche Analysen und maschinelles Lernen.","children":[],"payload":{"tag":"li","lines":"312,313"}},{"content":"<strong>IBM Cloud</strong>: Spark-Integration in IBM Cloud Pak for Data, ideal f&#xfc;r Hybrid-Cloud-Umgebungen und AI-Integration.","children":[],"payload":{"tag":"li","lines":"313,314"}},{"content":"<strong>Azure HDInsight</strong>: Managed Big Data Plattform von Microsoft f&#xfc;r Spark und andere Big Data-Tools.","children":[],"payload":{"tag":"li","lines":"314,315"}},{"content":"<strong>GCP (DataProc)</strong>: Google Cloud&#x2019;s verwaltete Spark- und Hadoop-Dienste f&#xfc;r skalierbare Datenverarbeitung.","children":[],"payload":{"tag":"li","lines":"315,316"}}],"payload":{"tag":"li","lines":"310,316"}},{"content":"<strong>BI Tools</strong>","children":[{"content":"Tableau","children":[],"payload":{"tag":"li","lines":"317,318"}},{"content":"Power BI","children":[],"payload":{"tag":"li","lines":"318,320"}}],"payload":{"tag":"li","lines":"316,320"}}],"payload":{"tag":"h2","lines":"305,306"}},{"content":"Libraries and Extensions","children":[{"content":"SparkR","children":[],"payload":{"tag":"li","lines":"321,322"}},{"content":"PySpark","children":[],"payload":{"tag":"li","lines":"322,323"}},{"content":"GraphFrames","children":[],"payload":{"tag":"li","lines":"323,324"}},{"content":"Koalas","children":[],"payload":{"tag":"li","lines":"324,326"}}],"payload":{"tag":"h2","lines":"320,321"}},{"content":"Spark Cluster Fehlerbehebung","children":[{"content":"Monitoring &amp; Debugging","children":[{"content":"Spark UI","children":[{"content":"Web-Interface zur &#xdc;berwachung von Jobs und deren Performance","children":[],"payload":{"tag":"li","lines":"330,331"}}],"payload":{"tag":"h4","lines":"329,330"}},{"content":"Event Logs","children":[{"content":"Detaillierte Logs, die zur Fehleranalyse und Performanceoptimierung genutzt werden","children":[],"payload":{"tag":"li","lines":"332,333"}}],"payload":{"tag":"h4","lines":"331,332"}},{"content":"Metrics","children":[{"content":"<strong>Ganglia</strong>","children":[],"payload":{"tag":"li","lines":"334,335"}},{"content":"<strong>Graphite</strong>","children":[],"payload":{"tag":"li","lines":"335,336"}}],"payload":{"tag":"h4","lines":"333,334"}},{"content":"Logging","children":[{"content":"<strong>Log4j</strong>","children":[],"payload":{"tag":"li","lines":"337,339"}}],"payload":{"tag":"h4","lines":"336,337"}}],"payload":{"tag":"h3","lines":"328,329"}},{"content":"Benutzercode","children":[{"content":"Fehler","children":[{"content":"<strong>NullPointerException</strong> / <strong>IndexOutOfBoundsException</strong>","children":[],"payload":{"tag":"li","lines":"341,342"}},{"content":"<strong>ClassNotFoundException</strong> / <strong>NoSuchMethodError</strong>","children":[],"payload":{"tag":"li","lines":"342,343"}},{"content":"<strong>SerializationException</strong>","children":[],"payload":{"tag":"li","lines":"343,344"}}],"payload":{"tag":"h5","lines":"340,341"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Debugging &amp; Logging</strong> (Spark-Logs, Stacktrace pr&#xfc;fen)","children":[],"payload":{"tag":"li","lines":"345,346"}},{"content":"<strong>Richtige Abh&#xe4;ngigkeiten einbinden</strong> (<code>--jars</code>, <code>--packages</code>)","children":[],"payload":{"tag":"li","lines":"346,347"}},{"content":"<strong>RDD-Transformationen pr&#xfc;fen</strong> (Lazy Evaluation beachten)","children":[],"payload":{"tag":"li","lines":"347,348"}},{"content":"<strong>Korrekte Serialisierung sicherstellen</strong> (<code>KryoSerializer</code> verwenden)","children":[],"payload":{"tag":"li","lines":"348,350"}}],"payload":{"tag":"h5","lines":"344,345"}}],"payload":{"tag":"h3","lines":"339,340"}},{"content":"System- &amp; Anwendungskonfiguration","children":[{"content":"Fehler","children":[{"content":"<strong>Inkompatible Spark- / Hadoop-Version</strong>","children":[],"payload":{"tag":"li","lines":"352,353"}},{"content":"<strong>Fehlende Umgebungsvariablen</strong> (<code>SPARK_HOME</code>, <code>HADOOP_CONF_DIR</code>)","children":[],"payload":{"tag":"li","lines":"353,354"}},{"content":"<strong>Unsachgem&#xe4;&#xdf;e Speicherverwaltung</strong>","children":[],"payload":{"tag":"li","lines":"354,355"}}],"payload":{"tag":"h5","lines":"351,352"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Versionen pr&#xfc;fen</strong> (<code>spark-submit --version</code>, <code>hadoop version</code>)","children":[],"payload":{"tag":"li","lines":"356,357"}},{"content":"<strong>Umgebungsvariablen setzen</strong>","children":[],"payload":{"tag":"li","lines":"357,358"}},{"content":"<strong>Speicherlimits optimieren</strong> (<code>spark.driver.memory</code>, <code>spark.executor.memory</code>)","children":[],"payload":{"tag":"li","lines":"358,360"}}],"payload":{"tag":"h5","lines":"355,356"}}],"payload":{"tag":"h3","lines":"350,351"}},{"content":"Fehlende oder falsche Anwendungsversion","children":[{"content":"Fehler","children":[{"content":"<strong>Nicht gefundene Bibliotheken</strong> (<code>NoClassDefFoundError</code>)","children":[],"payload":{"tag":"li","lines":"362,363"}},{"content":"<strong>Inkompatible JAR-Dateien</strong>","children":[],"payload":{"tag":"li","lines":"363,364"}}],"payload":{"tag":"h5","lines":"361,362"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Richtige Abh&#xe4;ngigkeiten sicherstellen</strong> (<code>--jars</code>, <code>--files</code>)","children":[],"payload":{"tag":"li","lines":"365,366"}},{"content":"<strong>Versionen von Bibliotheken abgleichen</strong>","children":[],"payload":{"tag":"li","lines":"366,368"}}],"payload":{"tag":"h5","lines":"364,365"}}],"payload":{"tag":"h3","lines":"360,361"}},{"content":"Falsche Ressourcenzuweisung","children":[{"content":"Fehler","children":[{"content":"<strong>OutOfMemoryError</strong>","children":[],"payload":{"tag":"li","lines":"370,371"}},{"content":"<strong>Executor Lost</strong>","children":[],"payload":{"tag":"li","lines":"371,372"}},{"content":"<strong>Task Timeout</strong>","children":[],"payload":{"tag":"li","lines":"372,373"}}],"payload":{"tag":"h5","lines":"369,370"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Speicher &amp; Kerne anpassen</strong> (<code>spark.executor.memory</code>, <code>spark.executor.cores</code>)","children":[],"payload":{"tag":"li","lines":"374,375"}},{"content":"<strong>Dynamische Allokation aktivieren</strong> (<code>spark.dynamicAllocation.enabled=true</code>)","children":[],"payload":{"tag":"li","lines":"375,376"}},{"content":"<strong>Partitionierung optimieren</strong> (<code>repartition()</code>, <code>coalesce()</code>)","children":[],"payload":{"tag":"li","lines":"376,378"}}],"payload":{"tag":"h5","lines":"373,374"}}],"payload":{"tag":"h3","lines":"368,369"}},{"content":"Netzwerkprobleme","children":[{"content":"Fehler","children":[{"content":"<strong>Timeouts bei Daten&#xfc;bertragung</strong>","children":[],"payload":{"tag":"li","lines":"380,381"}},{"content":"<strong>Verlorene Cluster-Knoten</strong>","children":[],"payload":{"tag":"li","lines":"381,382"}},{"content":"<strong>Verbindungsprobleme zwischen Master &amp; Worker</strong>","children":[],"payload":{"tag":"li","lines":"382,383"}}],"payload":{"tag":"h5","lines":"379,380"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Netzwerkkonfiguration pr&#xfc;fen</strong> (<code>spark.network.timeout</code>)","children":[],"payload":{"tag":"li","lines":"384,385"}},{"content":"<strong>Heartbeat-Intervalle anpassen</strong> (<code>spark.executor.heartbeatInterval</code>)","children":[],"payload":{"tag":"li","lines":"385,386"}},{"content":"<strong>Fehlertoleranzmechanismen nutzen</strong> (<code>spark.task.maxFailures</code>)","children":[],"payload":{"tag":"li","lines":"386,389"}}],"payload":{"tag":"h5","lines":"383,384"}}],"payload":{"tag":"h3","lines":"378,379"}}],"payload":{"tag":"h2","lines":"326,327"}}],"payload":{"tag":"h1","lines":"10,11"}},{"colorFreezeLevel":2,"theme":"dark","maxWidth":300,"activeNode":{"placement":"center"}})</script>
</body>
</html>
